{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Notebook.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "1A8Oe4pGnJk6",
        "colab_type": "code",
        "outputId": "08916ee7-c6df-4c65-df70-6836689dc6c1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "!pip install torchsummary"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torchsummary in /usr/local/lib/python3.6/dist-packages (1.5.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wJTk0echpMOj",
        "colab_type": "code",
        "outputId": "a2d3904c-9a69-4f46-d481-317f6975d976",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!python main.py"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n",
            "170500096it [00:06, 28244447.12it/s]                   \n",
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n",
            "CUDA Available? True\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 16, 32, 32]             432\n",
            "              ReLU-2           [-1, 16, 32, 32]               0\n",
            "       BatchNorm2d-3           [-1, 16, 32, 32]              32\n",
            "           Dropout-4           [-1, 16, 32, 32]               0\n",
            "            Conv2d-5           [-1, 32, 32, 32]           4,608\n",
            "              ReLU-6           [-1, 32, 32, 32]               0\n",
            "       BatchNorm2d-7           [-1, 32, 32, 32]              64\n",
            "           Dropout-8           [-1, 32, 32, 32]               0\n",
            "            Conv2d-9           [-1, 32, 32, 32]           9,216\n",
            "             ReLU-10           [-1, 32, 32, 32]               0\n",
            "      BatchNorm2d-11           [-1, 32, 32, 32]              64\n",
            "          Dropout-12           [-1, 32, 32, 32]               0\n",
            "        MaxPool2d-13           [-1, 32, 16, 16]               0\n",
            "           Conv2d-14           [-1, 32, 16, 16]           9,216\n",
            "             ReLU-15           [-1, 32, 16, 16]               0\n",
            "      BatchNorm2d-16           [-1, 32, 16, 16]              64\n",
            "          Dropout-17           [-1, 32, 16, 16]               0\n",
            "           Conv2d-18           [-1, 32, 16, 16]           9,216\n",
            "             ReLU-19           [-1, 32, 16, 16]               0\n",
            "      BatchNorm2d-20           [-1, 32, 16, 16]              64\n",
            "          Dropout-21           [-1, 32, 16, 16]               0\n",
            "        MaxPool2d-22             [-1, 32, 8, 8]               0\n",
            "           Conv2d-23             [-1, 16, 8, 8]             512\n",
            "             ReLU-24             [-1, 16, 8, 8]               0\n",
            "      BatchNorm2d-25             [-1, 16, 8, 8]              32\n",
            "          Dropout-26             [-1, 16, 8, 8]               0\n",
            "           Conv2d-27             [-1, 64, 8, 8]           9,216\n",
            "             ReLU-28             [-1, 64, 8, 8]               0\n",
            "      BatchNorm2d-29             [-1, 64, 8, 8]             128\n",
            "          Dropout-30             [-1, 64, 8, 8]               0\n",
            "           Conv2d-31             [-1, 64, 8, 8]             576\n",
            "           Conv2d-32             [-1, 64, 8, 8]           4,096\n",
            "             ReLU-33             [-1, 64, 8, 8]               0\n",
            "      BatchNorm2d-34             [-1, 64, 8, 8]             128\n",
            "          Dropout-35             [-1, 64, 8, 8]               0\n",
            "        MaxPool2d-36             [-1, 64, 4, 4]               0\n",
            "           Conv2d-37             [-1, 32, 4, 4]           2,048\n",
            "             ReLU-38             [-1, 32, 4, 4]               0\n",
            "      BatchNorm2d-39             [-1, 32, 4, 4]              64\n",
            "          Dropout-40             [-1, 32, 4, 4]               0\n",
            "           Conv2d-41             [-1, 32, 4, 4]           9,216\n",
            "             ReLU-42             [-1, 32, 4, 4]               0\n",
            "      BatchNorm2d-43             [-1, 32, 4, 4]              64\n",
            "          Dropout-44             [-1, 32, 4, 4]               0\n",
            "           Conv2d-45             [-1, 16, 4, 4]           4,608\n",
            "             ReLU-46             [-1, 16, 4, 4]               0\n",
            "      BatchNorm2d-47             [-1, 16, 4, 4]              32\n",
            "          Dropout-48             [-1, 16, 4, 4]               0\n",
            "        AvgPool2d-49             [-1, 16, 1, 1]               0\n",
            "           Conv2d-50             [-1, 10, 1, 1]             170\n",
            "================================================================\n",
            "Total params: 63,866\n",
            "Trainable params: 63,866\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 3.44\n",
            "Params size (MB): 0.24\n",
            "Estimated Total Size (MB): 3.69\n",
            "----------------------------------------------------------------\n",
            "None\n",
            "EPOCH: 1\n",
            "Loss=1.1842360496520996 Batch_id=390 Accuracy=43.19: 100% 391/391 [00:18<00:00, 20.61it/s]\n",
            "\n",
            "Test set: Average loss: 1.3590, Accuracy: 5096/10000 (50.96%)\n",
            "\n",
            "EPOCH: 2\n",
            "Loss=1.2068450450897217 Batch_id=390 Accuracy=60.26: 100% 391/391 [00:18<00:00, 22.28it/s]\n",
            "\n",
            "Test set: Average loss: 1.0287, Accuracy: 6372/10000 (63.72%)\n",
            "\n",
            "EPOCH: 3\n",
            "Loss=0.865159809589386 Batch_id=390 Accuracy=66.52: 100% 391/391 [00:18<00:00, 20.79it/s]\n",
            "\n",
            "Test set: Average loss: 0.8911, Accuracy: 6819/10000 (68.19%)\n",
            "\n",
            "EPOCH: 4\n",
            "Loss=0.7055832743644714 Batch_id=390 Accuracy=69.96: 100% 391/391 [00:18<00:00, 20.93it/s]\n",
            "\n",
            "Test set: Average loss: 0.8028, Accuracy: 7143/10000 (71.43%)\n",
            "\n",
            "EPOCH: 5\n",
            "Loss=1.0014837980270386 Batch_id=390 Accuracy=72.99: 100% 391/391 [00:18<00:00, 20.78it/s]\n",
            "\n",
            "Test set: Average loss: 0.7700, Accuracy: 7301/10000 (73.01%)\n",
            "\n",
            "EPOCH: 6\n",
            "Loss=0.678528904914856 Batch_id=390 Accuracy=74.48: 100% 391/391 [00:18<00:00, 20.62it/s]\n",
            "\n",
            "Test set: Average loss: 0.7188, Accuracy: 7436/10000 (74.36%)\n",
            "\n",
            "EPOCH: 7\n",
            "Loss=0.7770476341247559 Batch_id=390 Accuracy=76.11: 100% 391/391 [00:18<00:00, 20.67it/s]\n",
            "\n",
            "Test set: Average loss: 0.6974, Accuracy: 7562/10000 (75.62%)\n",
            "\n",
            "EPOCH: 8\n",
            "Loss=0.6524834632873535 Batch_id=390 Accuracy=77.32: 100% 391/391 [00:19<00:00, 20.57it/s]\n",
            "\n",
            "Test set: Average loss: 0.6530, Accuracy: 7709/10000 (77.09%)\n",
            "\n",
            "EPOCH: 9\n",
            "Loss=0.5942230224609375 Batch_id=390 Accuracy=78.40: 100% 391/391 [00:18<00:00, 20.64it/s]\n",
            "\n",
            "Test set: Average loss: 0.6358, Accuracy: 7801/10000 (78.01%)\n",
            "\n",
            "EPOCH: 10\n",
            "Loss=0.5958305597305298 Batch_id=390 Accuracy=79.16: 100% 391/391 [00:18<00:00, 20.66it/s]\n",
            "\n",
            "Test set: Average loss: 0.6559, Accuracy: 7682/10000 (76.82%)\n",
            "\n",
            "EPOCH: 11\n",
            "Loss=0.7218073606491089 Batch_id=390 Accuracy=79.93: 100% 391/391 [00:18<00:00, 20.73it/s]\n",
            "\n",
            "Test set: Average loss: 0.6476, Accuracy: 7753/10000 (77.53%)\n",
            "\n",
            "EPOCH: 12\n",
            "Loss=0.5294157862663269 Batch_id=390 Accuracy=80.41: 100% 391/391 [00:18<00:00, 20.60it/s]\n",
            "\n",
            "Test set: Average loss: 0.6005, Accuracy: 7908/10000 (79.08%)\n",
            "\n",
            "EPOCH: 13\n",
            "Loss=0.4189222455024719 Batch_id=390 Accuracy=80.90: 100% 391/391 [00:19<00:00, 20.56it/s]\n",
            "\n",
            "Test set: Average loss: 0.5907, Accuracy: 7951/10000 (79.51%)\n",
            "\n",
            "EPOCH: 14\n",
            "Loss=0.5966432690620422 Batch_id=390 Accuracy=81.41: 100% 391/391 [00:19<00:00, 20.52it/s]\n",
            "\n",
            "Test set: Average loss: 0.5700, Accuracy: 8030/10000 (80.30%)\n",
            "\n",
            "EPOCH: 15\n",
            "Loss=0.7228354215621948 Batch_id=390 Accuracy=81.65: 100% 391/391 [00:18<00:00, 20.62it/s]\n",
            "\n",
            "Test set: Average loss: 0.5803, Accuracy: 8017/10000 (80.17%)\n",
            "\n",
            "EPOCH: 16\n",
            "Loss=0.42182454466819763 Batch_id=390 Accuracy=82.14: 100% 391/391 [00:19<00:00, 20.46it/s]\n",
            "\n",
            "Test set: Average loss: 0.5690, Accuracy: 8017/10000 (80.17%)\n",
            "\n",
            "EPOCH: 17\n",
            "Loss=0.5099764466285706 Batch_id=390 Accuracy=82.53: 100% 391/391 [00:18<00:00, 20.58it/s]\n",
            "\n",
            "Test set: Average loss: 0.5693, Accuracy: 8062/10000 (80.62%)\n",
            "\n",
            "EPOCH: 18\n",
            "Loss=0.4454210698604584 Batch_id=390 Accuracy=82.83: 100% 391/391 [00:18<00:00, 21.68it/s]\n",
            "\n",
            "Test set: Average loss: 0.5368, Accuracy: 8177/10000 (81.77%)\n",
            "\n",
            "EPOCH: 19\n",
            "Loss=0.5734712481498718 Batch_id=390 Accuracy=83.04: 100% 391/391 [00:19<00:00, 20.43it/s]\n",
            "\n",
            "Test set: Average loss: 0.5710, Accuracy: 8052/10000 (80.52%)\n",
            "\n",
            "EPOCH: 20\n",
            "Loss=0.4358084201812744 Batch_id=390 Accuracy=83.40: 100% 391/391 [00:18<00:00, 22.09it/s]\n",
            "\n",
            "Test set: Average loss: 0.5232, Accuracy: 8202/10000 (82.02%)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}